
###############################################################################
#                        Reference Sample Section
#
# The Reference Sample Section handles the configuration related with the
# objects used to find neighbors from. Two different types of reference samples
# can be used. The first is a directory build using the NNPZ tools, containing
# the reference sample in the NNPZ format (parameter reference_sample_dir_. The
# second is a reference catalog, containing photometry values for the same bands
# as the target catalog (ref_cat option). Note that only one of the two options
# can be used.
#
###############################################################################

#
# Reference Sample directory options:
#
# - reference_sample_dir
#       The directory containing the reference sample files
# - reference_sample_providers
#       Data providers
# - reference_sample_phot_file
#       The file containing the photometry of the reference sample
# - reference_sample_phot_filters
#       The filters of the reference sample photometry to be used. They must
#       have the same order with the ones defined by the target_filters option
# - reference_sample_out_mean_phot_filters
#       A list of filters from the reference sample photometry file to compute
#       the weighted mean photometry for
#

#reference_sample_dir = '/disk2/OU-PHZ/NNPZ/ref_sample_20180410'
#reference_sample_providers =  {
#    'PdzProvider': {'name': 'pdz', 'data': 'pdz_data_{}.npy'},
#    'SedProvider': {'name': 'sed', 'data': 'sed_data_{}.npy'},
#    'MontecarloProvider': {'name': 'pp', 'data': 'pp_data_{}.npy'}
#}
reference_sample_phot_file = '/disk2/OU-PHZ/NNPZ/ref_sample_20180410/F_nu_uJy.fits'
reference_sample_phot_filters = ['u', 'g', 'r', 'i', 'z', 'vis', 'Y', 'J', 'H']
reference_sample_out_mean_phot_filters = reference_sample_phot_filters

###############################################################################
#                       Target Catalog Section
#
# The Target Catalog Section handles the configuration related with the catalog
# to compute the photometric redshift for.
#
# Target catalog options:
#
# - target_catalog
#       The input photometric catalog, in FITS or ASCII format
# - target_catalog_filters
#       The columns of the target catalog containing the photometry values. They
#       must have the same order defined by the reference_sample_phot_filters option
# - enable_filters
#       Enable a subset of the filters. Accept a list of glob patterns such as "lsst/*;euclid/*"
# - target_catalog_gal_ebv
#       The column of the target catalog containing the galactic E(B-V)
# - dust_map_sed_bpc
#       The band pass correction for the SED used for defining the dust column density map
# - target_catalog_filters_shifts
#       A dictionary where the key is the band name, and the value is the column on the
#       target catalog containing the shift of the average wavelength of the part of the filter that
#       influenced the measure.
# - missing_photometry_flags
#       A list containing all the values indicating missing data. Note that NaN
#       is implicitly translated to missing data.
# - input_size
#       Defines the number of rows of the input catalog to process
# - input_chunk_size
#       Process the input in chunks. This can be used to limit the memory consumption,
#       at the cost of some higher processing time, specially when running on slow I/O.
#
###############################################################################

target_catalog = '/disk2/OU-PHZ/DC3/jean/DC3.2.0/cats/euclid_emulated_DC3.2.0_COSMOS15_test_vis_detected.fits'
target_catalog_filters = [
    ('u_obs', 'u_obs_err'),
    ('g_obs', 'g_obs_err'),
    ('r_obs', 'r_obs_err'),
    ('i_obs', 'i_obs_err'),
    ('z_obs', 'z_obs_err'),
    ('vis_obs', 'vis_obs_err'),
    ('Y_obs', 'Y_obs_err'),
    ('J_obs', 'J_obs_err'),
    ('H_obs', 'H_obs_err')
]
#enable_filters = 'u;g;r;i'
#target_catalog_id_column = 'ID'
#target_catalog_gal_ebv = 'GAL_EBV'
#dust_map_sed_bpc = 1.018
#target_catalog_filters_shifts = {
#    'vis': 'AVG_TRANS_WAVE_VIS',
#    'Y': 'AVG_TRANS_WAVE_Y',
#    'J': 'AVG_TRANS_WAVE_J',
#    'H': 'AVG_TRANS_WAVE_H'
#}
missing_photometry_flags = [-99]
#input_size = 1000
input_chunk_size = 5120

###############################################################################
#                     NNPZ Algorithm Options Section
#
# This section contains the options related with the NNPZ algorithm configuration.
#
# Neighbors selection options:
#
# - neighbor_method
#       The method to be used for selecting the neighbors. It can be one of:
#       - KDTree: Fastest method, finding the neighbors using Euclidean distance.
#           WARNING: All errors are ignored when this method is used.
#       - BruteForce: Finds the neighbors using chi2 distance, by computing the
#           chi2 of all objects in the reference sample. WARNING: This method
#           is much slower than the alternatives.
#       - Combined: This method first finds a batch of neighbors in Euclidean
#           distance using a KDTree and then it finds the closest neighbors inside
#           the batch, by using chi2 distance.
# - neighbors_no
#       The number of neighbors to be used
# - scale_prior
#       If BruteForce is enabled, scale_prior can be used to allow the scaling of the reference
#       fluxes. It can be either the string 'uniform' (equivalent to angular distance),
#       'delta d' (a delta function at d), or 'tophat start begin' (clip the scaling to start begin).
#       Note that 'delta 1' would be equivalent to just disabling the scaling.
# - scale_max_iter
#       Maximum number of iterations to perform for the minimization of the posterior of the scale factor
# - scale_rtol
#       Tolerance (relative) for termination.
# - batch_size
#       The size of the batch size when the 'Combined' method is used.
#
# Weight calculation options:
#
# - weight_method
#       The method to be used for computing the weights of the neighbors. It can
#       be one of:
#       - Euclidean: The inversed Euclidean distance (errors are ignored)
#       - Chi2: The inversed chi2
#       - Likelihood: The likelihood computed as exp(-chi2 / 2)
# - weight_method_alternative
#       If weight_method yields a value of 0 for *all* neighbors of a given object,
#       this method will be used instead to recompute alternative weights.
#       It supports the same values as weight_method
# - balanced_kdtree
#       By default, the KDTree will be built using the median, creating a more compact
#       tree. It takes longer, but queries are faster.
#       In rare cases, depending on the data distribution (i.e a lot of zeros),
#       the algorithm may perform very poorly. In those cases, this could be disabled.
# - gpu_threads
#       If a GPU is available for the brute-force search use this many threads.
#       It is recommended to specify a multiple of 32 (corresponding to a warp size).
# - force_cpu
#       Force the execution of the brute-force algorithm on CPU even if there is
#       a GPU available.
#
###############################################################################

neighbor_method = 'Combined'
# scale_prior = 'uniform'
# scale_max_iter = 20
# scale_rtol = 1e-8
batch_size = 2000
neighbors_no = 30
weight_method = 'Likelihood'
weight_method_alternative = 'Chi2'
balanced_kdtree = True

#gpu_threads = 128
#force_cpu = True

###############################################################################
#                      Output Section
#
# The Output Section handles the configuration related with the produced output.
#
# Available options:
#
# - output_file
#       The file to store the output catalog
# - copy_input_columns
#       If True, the target catalog columns are copied to the output
#       If False, at least 'target_catalog_id_column' is copied to the output
# - neighbor_info_output
#       Add to the output catalog the reference sample neighbor IDs and weights
# - pdz_output
#       If False, the output catalog will skip the PDZ columns. They can later be
#       computed using NnpzConstructPDZ. Note that this flag being False only makes sense if
#       neighbor_info_output is True, otherwise all information is lost.
# - pdz_quantiles
#       A list with the PDZ quantiles to be added to the output
# - pdz_mc_samples
#       The number of Monte Carlo samples to get from the PDZ for each source
# - flags_in_separate_columns
#       When True, the catalog will contain a column for each flag. When False,
#       the flags wil be compressed by 8 per column.
# - pdz_point_estimates
#       A list of point estimates to add to the output. Supported: median, mean
#       and mode.
#
###############################################################################

output_file = '/disk2/OU-PHZ/DC3/nnpz/out.fits'
copy_input_columns = True
neighbor_info_output = True
pdz_output = True
pdz_quantiles = [0.25, 0.5, 0.75]
pdz_mc_samples = 1
flags_in_separate_columns = False
pdz_point_estimates = ['median']

###############################################################################
#                               Uniform Photometry
#
# NNPZ can output what is called a "uniform photometry":
# The photometry of the reference sample exists on an "idealized" color space,
# without galaxy reddening (or with a fixed, common, one) and without effects
# due to changes on the filter transmissions. Therefore: all sources within
# the reference sample are "comparable" between them.
# Sources from the target catalog, however, exist on their "own" color space.
# Each source is affected by a particular MW reddening - dependant on their location on the sky -
# and on a particular effect on the filter transmission - dependant on their location on the focal
# plane. The effect is small, but relevant.
# The objective of the Uniform Photometry is to "homogenize" the photometry of the target
# objects, and "bring them back" to the idealized reference color space.
#
# There are two options in NNPZ for this:
# 1. Output the weighted mean the photometry of the neighbors for a source
#       i.e, the "uniform" VIS is the _weighted_ mean of the VIS flux for all matched neighbor
#       This will always work, but does *not* take into account the actual photometry of the
#       target object. Any bias on the reference sample (i.e. SEDs used to compute the fluxes)
#       may propagate.
# 2. Correct the measured flux, "undoing" the effect of the reddening and filter variation
#       i.e. we take the "idealized" flux from the reference neighbors, and the flux recomputed
#       on the target object color space (with reddening and filter variation).
#       The ratio between both is computed, and used to correct the target measured flux:
#           f'_target = f_target * (f'_ref / f_ref)
#       Where f' is the "idealized" band.
#       This only makes sense if at least one (or both) of `target_catalog_gal_ebv` and
#       `target_catalog_filters_mean` are configured.
#       Nothe that f' and f may very well be two different, but close, bands: i.e. DES_g and LSST_g,
#       so this can also be used to compute "uniform" photometries for filters that have *not*
#       been measured.
#       Of course, this will *not* work if the target object flux is missing (LSST_u on the south)
#       In that case, the first option (weighted mean) may be used instead.
#
# Options:
# - redden_mean_phot
#       If true, output mean photometry will be reddened back to the target color space.
#       Requires a reference sample and target_catalog_gal_ebv to be set
# - corrected_photometry
#       A dictionary, where the key is a tuple with the names for the output (flux, error) columns,
#       and the value a tuple with four elements, where the first two are used to compute the
#       correction ratio, and the last two are the flux and error to be corrected
#           1. output uniform filter name (reference sample name)
#           2. idealized uniform filter name (reference sample name)
#           3. observed flux (target catalog name), must correspond to the filter in 2
#           4. observed flux error (target catalog name)
#
###############################################################################

redden_mean_phot = False

corrected_photometry = {
    # Generate uniform LSST G band from a corrected observed LSST G band
    ('g_corrected', 'g_corrected_err'): ('lsst_g', 'lsst_g', 'lsst_g_obs', 'lsst_g_obs_err'),
    # Generate uniform LSST G band from a corrected observed LSST R band
    ('g_r_corrected', 'g_r_corrected_err'): ('lsst_g', 'lsst_r', 'lsst_r_obs', 'lsst_r_obs_err'),
}

###############################################################################
#                      1D and 2D PDF from a weighted random sample
#
# NNPZ can generate marginalized one and two dimensional PDF from a multidimensional
# PDF via sampling:
# Each reference object has N random samples from a given k-dimensional PDF, so to compute
# the k-dimensional PDF associated to a target object we do a random weighted sampling
# from all the samples associated to all the neighbors.
#
# Available options:
#
# - mc_1d_pdf
#       A dictionary of list of tuples. The key *must* correspond to the
#       name of a MontecarloProvider registered with `reference_sample_providers`.
#       The value is a list of tuples of the form ('parameter name', binning_edges)
# - mc_2d_pdf
#       A dictionary of list of tuples. The key *must* correspond to the
#       name of a MontecarloProvider registered with `reference_sample_providers`.
#       The value is a list of tuples of the form
#       ('parameter 1', 'parameter 2', binning_edges_for_param_1, binning_edges_for_param_2)
# - mc_count
#       A dictionary of list of strings. The key *must* correspond to the
#       name of a MontecarloProvider registered with `reference_sample_providers`.
#       The value is a list of tuples of the form ('parameter name', 'binning center')
#       The functionality is similar to `mc_1d_pdf`, but it is intended for integer types.
# - mc_samples
#       A dictionary of list of tuples. The key *must* correspond to the
#       name of a MontecarloProvider registered with `reference_sample_providers`.
#       The value is a list of tuples of parameter names. If empty, or None, all known
#       parameters will be extracted.
# - mc_slice_aggregate
#       A dictionary of list of tuples. The key *must* correspond to the
#       name of a MontecarloProvider registered with `reference_sample_providers`.
#       The value is a list of tuples of the form
#       (target parameter, slice parameter, aggregate function, slice bin edges)
#       This is, for each slice of `slice parameter`, `target parameter` is aggregated
#       using `aggregate function` (i.e. np.nanmean, or np.mean if you know there are no NaN
#       on the reference sample)
# - mc_pdf_take_n
#       How many random samples to take from the samples associated to the neighbors
#
# Note: For a two dimensional PDF, the output has a *single* dimension, with the values
#       of each cell ravelled. The corresponding coordinates are stored on a separate
#       HDU prefixed `MC_PDF_2D_`, containing two columns with the corresponding 2D coordinates
#
###############################################################################

# mc_1d_pdf = {
#     'pp': [
#         ('Z', np.linspace(0, 6, 600)),
#         ('LOG_STELLARMASS', np.linspace(0, 12 ,100)),
#         ('SFR', np.linspace(-13, 8, 100)),
#     ]
# }
# mc_2d_pdf = {
#      'pp': [
#          ('Z', 'LOG_STELLARMASS', np.linspace(0, 6, 100), np.linspace(0,12,100)),
#          ('Z', 'SFR', np.linspace(0, 6, 100), np.linspace(-13, 8, 100))
#      ]
# }
# mc_count = {
#     'pp': [('SFH_TYPE', np.arange(0, 6))]
# }
# mc_samples = {
#    'pp': [
#         ('SFH_TYPE', ), ('GAL_TYPE',)
#     ]
# }
# mc_slice_aggregate = {
#    'pp': [
#         ('SFH_TAU', 'SFH_TYPE', np.arange(0, 7, dtype=np.float)-0.5, {'AVG': np.nanmean})
#    ]
# }
# mc_pdf_take_n = 500

###############################################################################
log_level = 'INFO'
